{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from postal.expand import expand_address\n",
    "import numpy as np \n",
    "from helpers import s3_connection\n",
    "import helpers \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/onyxia/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 KB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23 in /home/onyxia/.local/lib/python3.10/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/onyxia/.local/lib/python3.10/site-packages (from matplotlib) (24.2)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.56.0 kiwisolver-1.4.8 matplotlib-3.10.1 pillow-11.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2127436328.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install statsmodels\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DPE ADEME**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGEMENTS EXISTANTS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4537525"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/Users/noelinecasteil/Documents/statapp/DPE/DPE_ADEME/dpe-v2-logements-existants.csv\",\n",
    "    sep=\",\",  # Séparateur CSV\n",
    "    encoding=\"utf-8\",\n",
    "    low_memory=False)\n",
    "\n",
    "df['Date_réception_DPE'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import données isaure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connection successful\n"
     ]
    }
   ],
   "source": [
    "s3 = s3_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/STATAPP/helpers.py:35: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_in, usecols=columns_to_select, dtype=dtype_spec, sep=sep)\n"
     ]
    }
   ],
   "source": [
    "path_logements_existants = \"clichere/diffusion/DPE/DPE_ADEME/dpe-v2-logements-existants.csv\"\n",
    "df = s3.read_file_from_s3(path_logements_existants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre pour n'avoir que les données du département 44 en 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date_réception_DPE'] = pd.to_datetime(df['Date_réception_DPE'], errors='coerce')\n",
    "dfv1 = df[df['Date_réception_DPE'].dt.year == 2022].copy()\n",
    "dfv2 = dfv1[dfv1['N°_département_(BAN)']=='44'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(64490)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfv2['Date_réception_DPE'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction normalize renvoie des listes de différentes versions d'adresses possibles (ex Chateau-Thabut ; Chateau Thabut ; ChateauThabut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec cette version de la fonction on ne garde que la première composante de la liste. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_address(address):\n",
    "    if pd.isna(address) or address.strip() == '':\n",
    "        return None  \n",
    "    try:\n",
    "        normalized = expand_address(address)  \n",
    "        return normalized[0] if normalized else None  # Ne garde que la première version\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec l'adresse '{address}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfv2['Adresse_Normalisee'] = dfv2['Adresse_(BAN)'].apply(normalize_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGEMENTS NEUFS ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537952"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\n",
    "    \"/Users/noelinecasteil/Documents/statapp/DPE/DPE_ADEME/dpe-v2-logements-neufs.csv\",\n",
    "    sep=\",\",  # Séparateur CSV\n",
    "    encoding=\"utf-8\",\n",
    "    low_memory=False)\n",
    "\n",
    "df2['Date_réception_DPE'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import fichier logement neuf isaure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/STATAPP/helpers.py:35: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(file_in, usecols=columns_to_select, dtype=dtype_spec, sep=sep)\n"
     ]
    }
   ],
   "source": [
    "path_logements_neufs = \"clichere/diffusion/DPE/DPE_ADEME/dpe-v2-logements-neufs.csv\"\n",
    "df2 = s3.read_file_from_s3(path_logements_neufs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date_réception_DPE'] = pd.to_datetime(df2['Date_réception_DPE'], errors='coerce')\n",
    "df2v1 = df2[df2['Date_réception_DPE'].dt.year == 2022].copy()\n",
    "df2v2 = df2v1[df2v1['N°_département_(BAN)']=='44'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(11624)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2v2['Date_réception_DPE'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2v2['Adresse_Normalisee'] = df2v2['Adresse_(BAN)'].apply(normalize_address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALEURS FONCIERES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vf = pd.read_csv(\n",
    "    \"/Users/noelinecasteil/Documents/statapp/ValeursFoncieres/valeursfoncieres-2022.txt\",\n",
    "    sep=\"|\",  \n",
    "    encoding=\"utf-8\",\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import données isaure valeurs foncières"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/STATAPP/helpers.py:35: DtypeWarning: Columns (18,23,24,26,28,29,31,33,41) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_in, usecols=columns_to_select, dtype=dtype_spec, sep=sep)\n"
     ]
    }
   ],
   "source": [
    "path_valeursfoncieres_2022 = \"clichere/diffusion/Valeursfoncières/valeursfoncieres-2022.txt\"\n",
    "vf = s3.read_file_from_s3(path_valeursfoncieres_2022, sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No voie         float64\n",
      "Type de voie     object\n",
      "Voie             object\n",
      "Code postal     float64\n",
      "Commune          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(vf[['No voie', 'Type de voie', 'Voie', 'Code postal', 'Commune']].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir explicitement toutes les colonnes en chaînes\n",
    "vf['Adresse'] = vf['No voie'].apply(lambda x: str(int(x)) if pd.notna(x) else '').astype(str) + \" \" + \\\n",
    "                vf['Type de voie'].fillna('').astype(str) + \" \" + \\\n",
    "                vf['Voie'].fillna('').astype(str) + \", \" + \\\n",
    "                vf['Code postal'].apply(lambda x: str(int(x)) if pd.notna(x) else '').astype(str) + \" \" + \\\n",
    "                vf['Commune'].fillna('').astype(str)\n",
    "\n",
    "vf['Adresse'] = vf['Adresse'].str.strip().replace(r'^\\s*$', None, regex=True)  # Supprime les adresses vides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf44 = vf[vf['Code postal'].notna()]\n",
    "vf44 = vf44[vf44['Code postal'].astype(str).str.startswith('44')].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf44['Adresse'] = vf44['Adresse'].str.strip().str.replace(r'\\s+', ' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vf44['Adresse_Normalisee'] = vf44['Adresse'].apply(normalize_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99874\n"
     ]
    }
   ],
   "source": [
    "print(len(vf44['Adresse_Normalisee']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **MATCHING ETUDE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(df):\n",
    "    adresse_counts = df['Adresse_Normalisee'].value_counts()\n",
    "    nb_adresses_uniques = (adresse_counts == 1).sum()\n",
    "    return nb_adresses_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(22972)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(dfv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(16910)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(vf44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'adresses dupliquées dans dfv2 (logements existants) : 7513\n",
      "Nombre d'adresses dupliquées dans df2v2 (logements neufs) : 707\n",
      "Nombre d'adresses dupliquées dans vf44 (valeurs foncières) : 20674\n"
     ]
    }
   ],
   "source": [
    "print(f\"Nombre d'adresses dupliquées dans dfv2 (logements existants) : {dfv2 ['Adresse_Normalisee'].value_counts().ge(2).sum()}\")\n",
    "print(f\"Nombre d'adresses dupliquées dans df2v2 (logements neufs) : {df2v2 ['Adresse_Normalisee'].value_counts().ge(2).sum()}\")\n",
    "print(f\"Nombre d'adresses dupliquées dans vf44 (valeurs foncières) : {vf44 ['Adresse_Normalisee'].value_counts().ge(2).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAVAIL SUR LES DOUBLONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOUBLONS DANS DOC LOGEMENTS EXISTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_duplicate_percentage(df, address_column):\n",
    "    \"\"\"\n",
    "    On calcule le pourcentage d'adresses en doublon + on en fait une liste\n",
    "    \"\"\"\n",
    "    total_count = len(df)\n",
    "    duplicate_counts = df[address_column].value_counts()\n",
    "    duplicate_addresses = duplicate_counts[duplicate_counts > 1].index.tolist()\n",
    "    duplicate_count = len(duplicate_addresses)\n",
    "    duplicate_percentage = (duplicate_count / total_count) * 100 if total_count > 0 else 0\n",
    "    \n",
    "    return duplicate_percentage, duplicate_addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage d'adresses en doublon : 11.65%\n",
      "Liste des adresses en doublon :\n",
      "['1 rue de cahors 44800 saint-herblain', '5 avenue robert chasteland 44700 orvault', '32 route de la joneliere 44300 nantes', '24 rue blaise pascal 44300 nantes', 'route de saint joseph 44300 nantes', '4 avenue des jades 44300 nantes', 'rue de la coran 44400 reze', '8 place francois ii 44200 nantes', '129 rue de la mirette 44400 reze', '5 rue de biarritz 44200 nantes']\n",
      "7513\n"
     ]
    }
   ],
   "source": [
    "# On applique la fonction aux logements existants de dfv2\n",
    "duplicate_percentage, duplicate_addresses = calculate_duplicate_percentage(dfv2, 'Adresse_Normalisee')\n",
    "print(f\"Pourcentage d'adresses en doublon : {duplicate_percentage:.2f}%\")\n",
    "print(\"Liste des adresses en doublon :\")\n",
    "print(duplicate_addresses[:10])\n",
    "print(len(duplicate_addresses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DOUBLONS DANS VF44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage d'adresses en doublon : 20.70%\n",
      "Liste des adresses en doublon :\n",
      "['13 rue de saint servan 44800 st-herblain', 'rue de la jaunaie 44230 saint sebastien sur loire', 'rue joshua slocum 44210 pornic', '51 rue hector berlioz 44300 nantes', 'zone industrielle de brais 44600 saint-nazaire', '43 boulevard des batignolles 44300 nantes', '66 rue de nantes 44830 bouaye', 'favet 44650 corcoue-sur-logne', '95 avenue de la patouillerie 44700 orvault', 'le butay 44140 montbert']\n",
      "20674\n"
     ]
    }
   ],
   "source": [
    "# On applique la fonction aux logements existants de vf44\n",
    "duplicate_percentage2, duplicate_addresses2 = calculate_duplicate_percentage(vf44, 'Adresse_Normalisee')\n",
    "print(f\"Pourcentage d'adresses en doublon : {duplicate_percentage2:.2f}%\")\n",
    "liste2 = duplicate_addresses2\n",
    "print(\"Liste des adresses en doublon :\")\n",
    "print(liste2[:10])\n",
    "print(len(liste2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON CHERCHE LES ADRESSES A MATCH PARMI CES DOUBLONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1881\n"
     ]
    }
   ],
   "source": [
    "set_adresses = set(duplicate_addresses)\n",
    "set_adresses2 = set(duplicate_addresses2)\n",
    "# Adresses en commun\n",
    "adresses_match = set_adresses.intersection(set_adresses2)\n",
    "print(len(adresses_match))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donc sur toutes les adresses en doublons il y aurait seulement 1881 potentiellement à match. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule la perte potentielle si on venait à enlever tous les doublons et ne garder que les adresses uniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164364\n",
      "39882\n",
      "24.2644374680587\n"
     ]
    }
   ],
   "source": [
    "longueur = len(vf44)+len(dfv2)\n",
    "print(longueur)\n",
    "match_adresses_uniques = unique(dfv2)+unique(vf44)\n",
    "print(match_adresses_uniques)\n",
    "print(match_adresses_uniques*100/longueur)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On perdrait donc environ 24% de l'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATION DUNE LISTE AVEC LES ADRESSES UNIQUES ET EN DOUBLON POUR COMPTER NOMBRE DE MATCHING A EFFECTUER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des adresses uniques + adresses en doublon (mais une seule fois) :\n",
      "['16 rue du domaine 44120 vertou', '5bis chemin des jaunais 44760 les moutiers-en-retz', '3 betaie 44330 vallet', '18 le tertre 44260 prinquiau', '22 rue des moulins 44190 getigne', '34 rue du redois 44730 saint-michel-chef-chef', '5 la nouasse 44290 guemene-penfao', '20 rue jules meline 44300 nantes', '111 rue des deportes 44230 saint-sebastien-sur-loire', '4 rue andre etage roger perruche 44570 trignac']\n",
      "30485\n"
     ]
    }
   ],
   "source": [
    "adresse_counts = dfv2['Adresse_Normalisee'].value_counts()\n",
    "\n",
    "# Séparer les adresses uniques et celles en doublon (qu'on ne garde qu'une seule fois)\n",
    "adresses_uniques = adresse_counts[adresse_counts == 1].index.tolist()\n",
    "adresses_doublons = adresse_counts[adresse_counts > 1].index.tolist()\n",
    "\n",
    "# Fusionner les deux listes\n",
    "adresses_finales = adresses_uniques + adresses_doublons\n",
    "\n",
    "# Affichage du résultat\n",
    "print(\"Liste des adresses uniques + adresses en doublon (mais une seule fois) :\")\n",
    "print(adresses_finales[:10])\n",
    "print(len(adresses_finales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille finale de la liste : 37584\n",
      "Liste des adresses uniques + adresses en doublon (mais une seule fois) :\n",
      "['31 rue parpaillon 4400 barcelonnette', '7164 les jourdans 4400 saint-pons', 'les gaillards 4400 uvernet-fours', '36 chemin de soniers 4400 enchastrayes', 'roubine 4410 puimoisson', 'residence le chanteclerc 4400 uvernet-fours', '609 avenue du sauze 4400 enchastrayes', '9 lotissement du chazelas 4400 barcelonnette', '6031 les pedras 44117 saint-andre-des-eaux', 'les pedras 44117 saint-andre-des-eaux']\n"
     ]
    }
   ],
   "source": [
    "adresse_counts = vf44['Adresse_Normalisee'].dropna().value_counts()\n",
    "\n",
    "\n",
    "adresses_uniques = list(adresse_counts[adresse_counts == 1].index)\n",
    "adresses_doublons = list(adresse_counts[adresse_counts > 1].index)\n",
    "\n",
    "adresses_finales2 = adresses_uniques + adresses_doublons\n",
    "\n",
    "print(f\"Taille finale de la liste : {len(adresses_finales2)}\")\n",
    "print(\"Liste des adresses uniques + adresses en doublon (mais une seule fois) :\")\n",
    "print(adresses_finales2[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'adresses en commun : 7120\n",
      "Exemples d'adresses en commun : ['15 rue kervegan 44000 nantes', '32 rue de coulmiers 44000 nantes', '16 rue du four 44420 la turballe', '5 rue santeuil 44000 nantes', '22 route de la croix chevalier 44550 montoir-de-bretagne', '1 rue de la harviere 44450 divatte-sur-loire', '17 avenue de tremeac 44500 la baule-escoublac', '9 rue de la bastille 44000 nantes', '9 avenue du gosquet 44500 la baule-escoublac', '17 rue de la loire 44470 mauves-sur-loire']\n"
     ]
    }
   ],
   "source": [
    "# Conversion des listes en ensemble\n",
    "set_adresses_finales = set(adresses_finales)\n",
    "set_adresses_finales2 = set(adresses_finales2)\n",
    "\n",
    "# Adresses en commun\n",
    "adresses_communes = set_adresses_finales.intersection(set_adresses_finales2)\n",
    "\n",
    "print(f\"Nombre d'adresses en commun : {len(adresses_communes)}\")\n",
    "\n",
    "# Exemple d'adresses en commun\n",
    "print(\"Exemples d'adresses en commun :\", list(adresses_communes)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MEILLEURE VISION DES DOUBLONS EN CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention à ne pas push sur git le fichier csv généré (il est trop lourd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56901/2984065862.py:25: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  all_common_dupes = pd.concat([vf44_common, dfv2_common], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier CSV exporté : adresses_doublon_communes.csv\n"
     ]
    }
   ],
   "source": [
    "def export_duplicate_addresses(dfv2, vf44, output_file):\n",
    "    # Identifier les adresses en doublon dans vf44\n",
    "    vf44_dupes = vf44[vf44['Adresse_Normalisee'].duplicated(keep=False)]\n",
    "    \n",
    "    # Identifier les adresses en doublon dans dfv2\n",
    "    dfv2_dupes = dfv2[dfv2['Adresse_Normalisee'].duplicated(keep=False)]\n",
    "    \n",
    "    # Trouver les adresses en commun entre les deux jeux de données\n",
    "    common_addresses = set(vf44_dupes['Adresse_Normalisee']).intersection(set(dfv2_dupes['Adresse_Normalisee']))\n",
    "    \n",
    "    # Filtrer les données pour ne conserver que celles ayant une adresse en commun\n",
    "    vf44_common = vf44[vf44['Adresse_Normalisee'].isin(common_addresses)].copy()\n",
    "    dfv2_common = dfv2[dfv2['Adresse_Normalisee'].isin(common_addresses)].copy()\n",
    "    \n",
    "    # Ajouter une colonne Source pour identifier l'origine des données\n",
    "    vf44_common['Source'] = 'vf44'\n",
    "    dfv2_common['Source'] = 'dfv2'\n",
    "    \n",
    "    # Harmoniser les colonnes des deux dataframes\n",
    "    all_columns = list(set(dfv2_common.columns).union(set(vf44_common.columns)))\n",
    "    vf44_common = vf44_common.reindex(columns=all_columns)\n",
    "    dfv2_common = dfv2_common.reindex(columns=all_columns)\n",
    "    \n",
    "    # Concaténer les deux jeux de données sans perte d'information\n",
    "    all_common_dupes = pd.concat([vf44_common, dfv2_common], ignore_index=True)\n",
    "    \n",
    "    # Trier par adresse normalisée pour regrouper les doublons\n",
    "    all_common_dupes = all_common_dupes.sort_values(by=['Adresse_Normalisee'])\n",
    "    \n",
    "    # Exporter en CSV\n",
    "    all_common_dupes.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Fichier CSV exporté : {output_file}\")\n",
    "\n",
    "export_duplicate_addresses(dfv2, vf44, \"adresses_doublon_communes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque un problème avec les dépendances qui ont les mêmes infos que les appartements dans lesquels elles sont situées. On veut voir quelle quantité elles représentent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes avec le type de local 'Dépendance' dans vf44 : 26009\n",
      "99874\n"
     ]
    }
   ],
   "source": [
    "# Filtre\n",
    "dependance_count = vf44[vf44['Type local'] == 'Dépendance'].shape[0]\n",
    "\n",
    "print(f\"Nombre de lignes avec le type de local 'Dépendance' dans vf44 : {dependance_count}\")\n",
    "# Nombre total de lignes\n",
    "print(len(vf44['Type local']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREMIER TEST MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Date_réception_DPE', 'Date_établissement_DPE', 'Modèle_DPE',\n",
      "       'Date_fin_validité_DPE', 'Version_DPE', 'Méthode_application_DPE',\n",
      "       'Etiquette_DPE', 'Etiquette_GES', 'Année_construction', 'Type_bâtiment',\n",
      "       'Période_construction', 'Surface_habitable_logement', 'Adresse_brute',\n",
      "       'Nom__commune_(BAN)', 'Code_INSEE_(BAN)', 'N°_voie_(BAN)',\n",
      "       'Identifiant__BAN', 'Adresse_(BAN)', 'Code_postal_(BAN)', 'Score_BAN',\n",
      "       'Nom__rue_(BAN)', 'Coordonnée_cartographique_X_(BAN)',\n",
      "       'Coordonnée_cartographique_Y_(BAN)', 'Code_postal_(brut)',\n",
      "       'N°_étage_appartement', 'Nom_résidence', 'Cage_d'escalier',\n",
      "       'Complément_d'adresse_logement', 'Statut_géocodage',\n",
      "       'Nom__commune_(Brut)', 'N°_département_(BAN)', 'N°_région_(BAN)',\n",
      "       'Complément_d'adresse_bâtiment', 'Adresse_Normalisee'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(dfv2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Identifiant de document', 'Reference document', '1 Articles CGI',\n",
      "       '2 Articles CGI', '3 Articles CGI', '4 Articles CGI', '5 Articles CGI',\n",
      "       'No disposition', 'Date mutation', 'Nature mutation', 'Valeur fonciere',\n",
      "       'No voie', 'B/T/Q', 'Type de voie', 'Code voie', 'Voie', 'Code postal',\n",
      "       'Commune', 'Code departement', 'Code commune', 'Prefixe de section',\n",
      "       'Section', 'No plan', 'No Volume', '1er lot',\n",
      "       'Surface Carrez du 1er lot', '2eme lot', 'Surface Carrez du 2eme lot',\n",
      "       '3eme lot', 'Surface Carrez du 3eme lot', '4eme lot',\n",
      "       'Surface Carrez du 4eme lot', '5eme lot', 'Surface Carrez du 5eme lot',\n",
      "       'Nombre de lots', 'Code type local', 'Type local', 'Identifiant local',\n",
      "       'Surface reelle bati', 'Nombre pieces principales', 'Nature culture',\n",
      "       'Nature culture speciale', 'Surface terrain', 'Adresse',\n",
      "       'Adresse_Normalisee'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(vf44.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code prend 2min environ à tourner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_match(vf44,dfv2):\n",
    "    adresse_counts = vf44['Adresse_Normalisee'].dropna().value_counts()\n",
    "    unique1 = list(adresse_counts[adresse_counts == 1].index)\n",
    "    doublons1 = list(adresse_counts[adresse_counts > 1].index)\n",
    "    final = unique1 + doublons1\n",
    "    set_final = set(final)\n",
    "\n",
    "    adresse_counts2 = dfv2['Adresse_Normalisee'].dropna().value_counts()\n",
    "    unique2 = list(adresse_counts2[adresse_counts2 == 1].index)\n",
    "    doublons2 = list(adresse_counts2[adresse_counts2 > 1].index)\n",
    "    final2 = unique2 + doublons2\n",
    "    set_final2 = set(final2)\n",
    "\n",
    "    commun = set_final.intersection(set_final2)\n",
    "\n",
    "    vf44['Surface Carrez du 1er lot'] = pd.to_numeric(\n",
    "        vf44['Surface Carrez du 1er lot'].astype(str).str.replace(',', '.'), errors='coerce'\n",
    "    )\n",
    "    dfv2['Surface_habitable_logement'] = pd.to_numeric(\n",
    "        dfv2['Surface_habitable_logement'].astype(str).str.replace(',', '.'), errors='coerce'\n",
    "    )\n",
    "\n",
    "    merged = []\n",
    "\n",
    "    for adresse in commun:\n",
    "        dfv2sub = dfv2[dfv2['Adresse_Normalisee'] == adresse]\n",
    "        vf44sub = vf44[vf44['Adresse_Normalisee'] == adresse]\n",
    "\n",
    "        if len(dfv2sub)==1 and len(vf44sub)==1:\n",
    "            merged.append({**dfv2sub.iloc[0].to_dict(), **vf44sub.iloc[0].to_dict()})\n",
    "        else :\n",
    "             # Boucle sur les éléments de dfv2sub\n",
    "            for _, row2 in dfv2sub.iterrows():\n",
    "                matched = False\n",
    "                for _, row1 in vf44sub.iterrows():\n",
    "                    surface1 = row1['Surface Carrez du 1er lot']\n",
    "                    surface2 = row2['Surface_habitable_logement']\n",
    "                    \n",
    "                    # Vérification de l'écart de surface\n",
    "                    if abs(surface1 - surface2) / max(surface1, surface2) < 0.05:\n",
    "                        merged.append({**row2.to_dict(), **row1.to_dict()})\n",
    "                        matched = True\n",
    "                        break\n",
    "                \n",
    "                if not matched:\n",
    "                    merged.append(row2.to_dict())\n",
    "            \n",
    "            # Ajouter les éléments restants de vf44sub s'ils ne sont pas associés\n",
    "            for _, row1 in vf44sub.iterrows():\n",
    "                if not any(row1['Adresse_Normalisee'] == m['Adresse_Normalisee'] for m in merged):\n",
    "                    merged.append(row1.to_dict())\n",
    "        \n",
    "    dfv2_uniques = dfv2[~dfv2['Adresse_Normalisee'].isin(commun)]\n",
    "    print(len(dfv2_uniques))\n",
    "    vf44_uniques = vf44[~vf44['Adresse_Normalisee'].isin(commun)]\n",
    "    print(len(vf44_uniques))\n",
    "\n",
    "    df = pd.DataFrame(merged)\n",
    "    df = pd.concat([df, dfv2_uniques, vf44_uniques], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "test = test_match(vf44,dfv2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142730\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64490\n",
      "99874\n"
     ]
    }
   ],
   "source": [
    "print(len(dfv2))\n",
    "print(len(vf44))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adresse commune trouvée : 20 rue du gue robert 44000 nantes\n",
      "Ok.\n"
     ]
    }
   ],
   "source": [
    "#adresses en doublons\n",
    "dfv2_dupes = dfv2[dfv2['Adresse_Normalisee'].duplicated(keep=False)]\n",
    "vf44_dupes = vf44[vf44['Adresse_Normalisee'].duplicated(keep=False)]\n",
    "\n",
    "#adresses communes aux deux fichiers\n",
    "common_addresses = set(dfv2_dupes['Adresse_Normalisee']).intersection(set(vf44_dupes['Adresse_Normalisee']))\n",
    "\n",
    "#on sélectionne une adresse commune\n",
    "if common_addresses:\n",
    "    adresse_commune = list(common_addresses)[0]  \n",
    "    print(f\"Adresse commune trouvée : {adresse_commune}\")\n",
    "    \n",
    "    #lignes correspondantes\n",
    "    dfv2_common = dfv2[dfv2['Adresse_Normalisee'] == adresse_commune]\n",
    "    vf44_common = vf44[vf44['Adresse_Normalisee'] == adresse_commune]\n",
    "    \n",
    "    #transfo csv\n",
    "    dfv2_common.to_csv(\"logements_existants_adresse_commune.csv\", index=False, encoding='utf-8')\n",
    "    vf44_common.to_csv(\"valeurs_foncieres_adresse_commune.csv\", index=False, encoding='utf-8')\n",
    "    \n",
    "    print(\"Ok.\")\n",
    "else:\n",
    "    print(\"Bug\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
